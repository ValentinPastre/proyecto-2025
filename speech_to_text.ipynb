{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValentinPastre/proyecto-2025/blob/Speech-to-Text/speech_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib librosa torch torchaudio torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxVs_RjJFARL",
        "outputId": "11181ce4-2ca6-4a35-8c61-8a47cbe24628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "cgXC07VweM9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "from sklearn.model_selection import train_test_split\n",
        "import soundfile as sf"
      ],
      "metadata": {
        "id": "uHXp6HI7FFJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "ds_f = load_dataset(\"ylacombe/google-argentinian-spanish\", \"female\")\n",
        "ds_m = load_dataset(\"ylacombe/google-argentinian-spanish\", \"male\")\n",
        "\n",
        "ds = concatenate_datasets([ds_f[\"train\"], ds_m[\"train\"]])\n",
        "\n",
        "print(ds.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLTe8S7zFf_w",
        "outputId": "8bac2e59-f020-42e9-c583-ef7f12df1ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['audio', 'text', 'speaker_id']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)\n",
        "\n",
        "splits = ds.train_test_split(test_size=0.1, seed=42)\n",
        "train_df = splits[\"train\"]\n",
        "val_df = splits[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSnvzlh7IakP",
        "outputId": "3b08bda3-8b44-4ad1-cd68-b0152c17be65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['audio', 'text', 'speaker_id'],\n",
            "    num_rows: 5739\n",
            "})\n",
            "Training samples: 5165, Validation samples: 574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nul6F3dijQU",
        "outputId": "7d340e72-393a-4336-e5d2-6636a2e8389f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Del subte al teatro son diez minutos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#char_map_str = \"\"\"\n",
        "#a 1\n",
        "#b 2\n",
        "#c 3\n",
        "#d 4\n",
        "#e 5\n",
        "#f 6\n",
        "#g 7\n",
        "#h 8\n",
        "#i 9\n",
        "#j 10\n",
        "#k 11\n",
        "#l 12\n",
        "#m 13\n",
        "#n 14\n",
        "#o 15\n",
        "#p 16\n",
        "#q 17\n",
        "#r 18\n",
        "#s 19\n",
        "#t 20\n",
        "#u 21\n",
        "#v 22\n",
        "#w 23\n",
        "#x 24\n",
        "#y 25\n",
        "#z 26\n",
        "#ñ 27\n",
        "#á 28\n",
        "#é 29\n",
        "#í 30\n",
        "#ó 31\n",
        "#ú 32\n",
        "#? 33\n",
        "#¿ 34\n",
        "#! 35\n",
        "#¡ 36\n",
        "#SPACE 37\n",
        "#\"\"\"\n",
        "#char_map = {}\n",
        "#index_map = {}\n",
        "#char_map['<blank>'] = 0\n",
        "#index_map[0] = ''\n",
        "#for line in char_map_str.strip().split('\\n'):\n",
        "#    ch, index = line.split()\n",
        "#    if ch == \"SPACE\":\n",
        "#        ch = \" \"\n",
        "#    index = int(index) + 1\n",
        "#    #char_map[ch] = int(index)\n",
        "#    char_map[ch] = index\n",
        "#    #index_map[int(index)] = ch\n",
        "#    index_map[index] = ch"
      ],
      "metadata": {
        "id": "b1yb6JuIO-8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = \" \".join(train_df['text']) + \" \" + \" \".join(val_df['text'])\n",
        "all_text = all_text.lower()\n",
        "\n",
        "allowed_chars = sorted(list(set(all_text)))  # todos los que realmente aparecen\n",
        "\n",
        "char_map = {\"<BLANK>\": 0}\n",
        "for i, ch in enumerate(allowed_chars, start=1):\n",
        "    char_map[ch] = i\n",
        "\n",
        "index_map = {v: k for k, v in char_map.items()}"
      ],
      "metadata": {
        "id": "VwrdzF5LAeyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_int_sequence(text):\n",
        "    text = text.lower()\n",
        "    return [char_map.get(c, char_map[' ']) for c in text]\n",
        "\n",
        "def int_sequence_to_text(seq):\n",
        "    #return ''.join([index_map[i] for i in seq])\n",
        "    return ''.join([index_map.get(i, '') for i in seq])"
      ],
      "metadata": {
        "id": "n4onsy3xQJEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[0][\"text\"])\n",
        "print(text_to_int_sequence(train_df[0][\"text\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD6bdWghjO2r",
        "outputId": "2f2f5e0c-3988-4b91-c65d-12c0d46db41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Del subte al teatro son diez minutos\n",
            "[12, 13, 20, 1, 27, 29, 10, 28, 13, 1, 9, 20, 1, 28, 13, 9, 28, 26, 23, 1, 27, 23, 22, 1, 12, 17, 13, 34, 1, 21, 17, 22, 29, 28, 23, 27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Custom Dataset"
      ],
      "metadata": {
        "id": "z06tsjT9eKCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, df, char_map, transform=None):\n",
        "        self.df = df\n",
        "        self.char_map = char_map\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.df[idx]\n",
        "        audio = item[\"audio\"][\"array\"]\n",
        "        sample_rate = item[\"audio\"][\"sampling_rate\"]\n",
        "        transcript = item[\"text\"]\n",
        "\n",
        "        waveform = torch.tensor(audio)\n",
        "        if waveform.dim() == 1:\n",
        "          waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Resample to 16kHz\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "            sample_rate = 16000\n",
        "\n",
        "        max_val = torch.max(torch.abs(waveform))\n",
        "        if max_val > 0:\n",
        "          waveform = waveform/max_val\n",
        "\n",
        "\n",
        "        spectrogram = self.transform(waveform)\n",
        "        spectrogram = torch.log(spectrogram + 1e-6)\n",
        "        spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
        "        spectrogram = spectrogram.squeeze(0).transpose(0, 1)\n",
        "\n",
        "        # Convert transcript to int sequence\n",
        "        transcript_seq = torch.tensor(text_to_int_sequence(transcript), dtype=torch.long)\n",
        "\n",
        "        return spectrogram, transcript_seq"
      ],
      "metadata": {
        "id": "XNVkLuLl7ubk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    spectrograms = []\n",
        "    transcript_seqs = []\n",
        "    input_lengths = []\n",
        "    target_lengths = []\n",
        "\n",
        "    for (spectrogram, transcript_seq) in batch:\n",
        "        if spectrogram.dim() != 2:\n",
        "            raise ValueError(f\"Expected spectrogram with 2 dims (T, n_mels), got {spectrogram.shape}\")\n",
        "\n",
        "        spectrograms.append(spectrogram)\n",
        "        transcript_seqs.append(transcript_seq)\n",
        "        input_lengths.append(spectrogram.shape[0])\n",
        "        target_lengths.append(len(transcript_seq))\n",
        "\n",
        "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
        "    transcript_seqs = torch.nn.utils.rnn.pad_sequence(transcript_seqs, batch_first=True)\n",
        "\n",
        "    return spectrograms, transcript_seqs, input_lengths, target_lengths"
      ],
      "metadata": {
        "id": "M2YDUcinag5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "\n",
        "#train_dataset = SpeechDataset(train_df.reset_index(drop=True), char_map, transform=transform)\n",
        "#val_dataset = SpeechDataset(val_df.reset_index(drop=True), char_map, transform=transform)\n",
        "train_dataset = SpeechDataset(train_df, char_map, transform=transform)\n",
        "val_dataset = SpeechDataset(val_df, char_map, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYmjG-ZTbc_q",
        "outputId": "22fc67d2-d347-4f50-c7c3-c5fec3d9f9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/functional/functional.py:582: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "for batch in train_loader:\n",
        "    spectrograms, targets, input_lengths, target_lengths = batch\n",
        "\n",
        "    spec = spectrograms[0]   # elegimos el primer espectrograma del batch (shape: [Time, Mel])\n",
        "\n",
        "    print(\"Shape:\", spec.shape)\n",
        "    print(\"Min:\", spec.min().item())\n",
        "    print(\"Max:\", spec.max().item())\n",
        "    print(\"Mean:\", spec.mean().item())\n",
        "    print(\"Std:\", spec.std().item())\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfStBjctuAAE",
        "outputId": "8476c969-10a1-4ea8-b3e7-b84247503cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([601, 128])\n",
            "Min: -1.5699036121368408\n",
            "Max: 2.5417768955230713\n",
            "Mean: -3.322388764104289e-08\n",
            "Std: 0.738749623298645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "8RDAtzMwd6-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=3, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Byz2F9XVd-0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"   En Colab: Runtime , Change runtime type , T4 GPU\")\n",
        "\n",
        "input_size = 128  # Number of Mel features\n",
        "hidden_size = 256\n",
        "output_size = len(char_map)  # Number of characters\n",
        "\n",
        "model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBbz6xVSebBl",
        "outputId": "0b5284b2-791d-48e8-c0c2-08ca61ce90dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "Memoria disponible: 14.74 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "_VdOtG94ekGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "j4o0z5nuegXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50  # Increase this number for better performance\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, targets, input_lengths, target_lengths) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        # CTC Loss expects (T, N, C)\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        input_lengths_tensor = torch.IntTensor(input_lengths)    # shape (N,)\n",
        "        target_lengths_tensor = torch.IntTensor(target_lengths)  # shape (N,)\n",
        "\n",
        "        targets_list = []\n",
        "        for t, l in zip(targets, target_lengths):\n",
        "            targets_list.append(t[:l])\n",
        "        targets_concat = torch.cat(targets_list).to(dtype=torch.long)  # 1D\n",
        "\n",
        "        outputs = outputs.to(device)               # (T, N, C)\n",
        "        targets_concat = targets_concat.to(device)\n",
        "        input_lengths_tensor = input_lengths_tensor.to(device)\n",
        "        target_lengths_tensor = target_lengths_tensor.to(device)\n",
        "\n",
        "        loss = criterion(outputs, targets_concat, input_lengths_tensor, target_lengths_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed with average loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slt-T5otepfc",
        "outputId": "e2b5b62d-15e1-4c2a-f5fd-f6d07027d1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Step 1/162, Loss: 29.8145\n",
            "Epoch 1/50, Step 11/162, Loss: 3.2776\n",
            "Epoch 1/50, Step 21/162, Loss: 3.1958\n",
            "Epoch 1/50, Step 31/162, Loss: 3.0839\n",
            "Epoch 1/50, Step 41/162, Loss: 3.0498\n",
            "Epoch 1/50, Step 51/162, Loss: 2.9827\n",
            "Epoch 1/50, Step 61/162, Loss: 3.0065\n",
            "Epoch 1/50, Step 71/162, Loss: 3.0162\n",
            "Epoch 1/50, Step 81/162, Loss: 2.9989\n",
            "Epoch 1/50, Step 91/162, Loss: 2.9837\n",
            "Epoch 1/50, Step 101/162, Loss: 2.9545\n",
            "Epoch 1/50, Step 111/162, Loss: 2.9823\n",
            "Epoch 1/50, Step 121/162, Loss: 2.9859\n",
            "Epoch 1/50, Step 131/162, Loss: 2.9355\n",
            "Epoch 1/50, Step 141/162, Loss: 2.9482\n",
            "Epoch 1/50, Step 151/162, Loss: 2.9579\n",
            "Epoch 1/50, Step 161/162, Loss: 2.9308\n",
            "Epoch 1 completed with average loss: 3.5318\n",
            "Epoch 2/50, Step 1/162, Loss: 2.9288\n",
            "Epoch 2/50, Step 11/162, Loss: 2.9326\n",
            "Epoch 2/50, Step 21/162, Loss: 2.9634\n",
            "Epoch 2/50, Step 31/162, Loss: 2.9507\n",
            "Epoch 2/50, Step 41/162, Loss: 2.9369\n",
            "Epoch 2/50, Step 51/162, Loss: 2.9363\n",
            "Epoch 2/50, Step 61/162, Loss: 2.9347\n",
            "Epoch 2/50, Step 71/162, Loss: 2.9733\n",
            "Epoch 2/50, Step 81/162, Loss: 2.9611\n",
            "Epoch 2/50, Step 91/162, Loss: 2.9461\n",
            "Epoch 2/50, Step 101/162, Loss: 2.9381\n",
            "Epoch 2/50, Step 111/162, Loss: 2.9327\n",
            "Epoch 2/50, Step 121/162, Loss: 2.9468\n",
            "Epoch 2/50, Step 131/162, Loss: 2.9035\n",
            "Epoch 2/50, Step 141/162, Loss: 2.9221\n",
            "Epoch 2/50, Step 151/162, Loss: 2.9217\n",
            "Epoch 2/50, Step 161/162, Loss: 2.9107\n",
            "Epoch 2 completed with average loss: 2.9282\n",
            "Epoch 3/50, Step 1/162, Loss: 2.9256\n",
            "Epoch 3/50, Step 11/162, Loss: 2.8920\n",
            "Epoch 3/50, Step 21/162, Loss: 2.9209\n",
            "Epoch 3/50, Step 31/162, Loss: 2.8527\n",
            "Epoch 3/50, Step 41/162, Loss: 2.8764\n",
            "Epoch 3/50, Step 51/162, Loss: 2.8720\n",
            "Epoch 3/50, Step 61/162, Loss: 2.8527\n",
            "Epoch 3/50, Step 71/162, Loss: 2.8445\n",
            "Epoch 3/50, Step 81/162, Loss: 2.7779\n",
            "Epoch 3/50, Step 91/162, Loss: 2.7727\n",
            "Epoch 3/50, Step 101/162, Loss: 2.7654\n",
            "Epoch 3/50, Step 111/162, Loss: 2.7467\n",
            "Epoch 3/50, Step 121/162, Loss: 2.7157\n",
            "Epoch 3/50, Step 131/162, Loss: 2.6917\n",
            "Epoch 3/50, Step 141/162, Loss: 2.6221\n",
            "Epoch 3/50, Step 151/162, Loss: 2.5319\n",
            "Epoch 3/50, Step 161/162, Loss: 2.4675\n",
            "Epoch 3 completed with average loss: 2.7671\n",
            "Epoch 4/50, Step 1/162, Loss: 2.5012\n",
            "Epoch 4/50, Step 11/162, Loss: 2.3979\n",
            "Epoch 4/50, Step 21/162, Loss: 2.3811\n",
            "Epoch 4/50, Step 31/162, Loss: 2.2728\n",
            "Epoch 4/50, Step 41/162, Loss: 2.2745\n",
            "Epoch 4/50, Step 51/162, Loss: 2.3120\n",
            "Epoch 4/50, Step 61/162, Loss: 2.1796\n",
            "Epoch 4/50, Step 71/162, Loss: 2.2455\n",
            "Epoch 4/50, Step 81/162, Loss: 2.0967\n",
            "Epoch 4/50, Step 91/162, Loss: 2.1247\n",
            "Epoch 4/50, Step 101/162, Loss: 2.1162\n",
            "Epoch 4/50, Step 111/162, Loss: 2.0528\n",
            "Epoch 4/50, Step 121/162, Loss: 1.9977\n",
            "Epoch 4/50, Step 131/162, Loss: 1.8240\n",
            "Epoch 4/50, Step 141/162, Loss: 1.7948\n",
            "Epoch 4/50, Step 151/162, Loss: 1.7314\n",
            "Epoch 4/50, Step 161/162, Loss: 1.8132\n",
            "Epoch 4 completed with average loss: 2.1135\n",
            "Epoch 5/50, Step 1/162, Loss: 1.7971\n",
            "Epoch 5/50, Step 11/162, Loss: 1.7176\n",
            "Epoch 5/50, Step 21/162, Loss: 1.6093\n",
            "Epoch 5/50, Step 31/162, Loss: 1.6549\n",
            "Epoch 5/50, Step 41/162, Loss: 1.4853\n",
            "Epoch 5/50, Step 51/162, Loss: 1.5576\n",
            "Epoch 5/50, Step 61/162, Loss: 1.5594\n",
            "Epoch 5/50, Step 71/162, Loss: 1.6020\n",
            "Epoch 5/50, Step 81/162, Loss: 1.6155\n",
            "Epoch 5/50, Step 91/162, Loss: 1.4924\n",
            "Epoch 5/50, Step 101/162, Loss: 1.5063\n",
            "Epoch 5/50, Step 111/162, Loss: 1.4929\n",
            "Epoch 5/50, Step 121/162, Loss: 1.4912\n",
            "Epoch 5/50, Step 131/162, Loss: 1.4311\n",
            "Epoch 5/50, Step 141/162, Loss: 1.3695\n",
            "Epoch 5/50, Step 151/162, Loss: 1.4092\n",
            "Epoch 5/50, Step 161/162, Loss: 1.3616\n",
            "Epoch 5 completed with average loss: 1.5571\n",
            "Epoch 6/50, Step 1/162, Loss: 1.3134\n",
            "Epoch 6/50, Step 11/162, Loss: 1.3170\n",
            "Epoch 6/50, Step 21/162, Loss: 1.3103\n",
            "Epoch 6/50, Step 31/162, Loss: 1.3850\n",
            "Epoch 6/50, Step 41/162, Loss: 1.2470\n",
            "Epoch 6/50, Step 51/162, Loss: 1.3259\n",
            "Epoch 6/50, Step 61/162, Loss: 1.1693\n",
            "Epoch 6/50, Step 71/162, Loss: 1.2145\n",
            "Epoch 6/50, Step 81/162, Loss: 1.1897\n",
            "Epoch 6/50, Step 91/162, Loss: 1.2480\n",
            "Epoch 6/50, Step 101/162, Loss: 1.1599\n",
            "Epoch 6/50, Step 111/162, Loss: 1.1693\n",
            "Epoch 6/50, Step 121/162, Loss: 1.1225\n",
            "Epoch 6/50, Step 131/162, Loss: 1.2064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate the model"
      ],
      "metadata": {
        "id": "J54lqfL9oDEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cer(prediction, reference):\n",
        "    if len(reference) == 0:\n",
        "        return 1.0 if len(prediction) > 0 else 0.0\n",
        "\n",
        "    pred_chars = list(prediction.replace(' ', ''))\n",
        "    ref_chars = list(reference.replace(' ', ''))\n",
        "\n",
        "    if len(pred_chars) == 0:\n",
        "        return 1.0\n",
        "\n",
        "    errors = 0\n",
        "    min_len = min(len(pred_chars), len(ref_chars))\n",
        "    max_len = max(len(pred_chars), len(ref_chars))\n",
        "\n",
        "    for i in range(min_len):\n",
        "        if pred_chars[i] != ref_chars[i]:\n",
        "            errors += 1\n",
        "\n",
        "    errors += (max_len - min_len)\n",
        "\n",
        "    return errors / len(ref_chars)"
      ],
      "metadata": {
        "id": "Vy2evuFOToaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_decode(indices, blank_index=0):\n",
        "    \"\"\"\n",
        "    indices: tensor shape (T,) con los índices ya argmaxeados.\n",
        "    \"\"\"\n",
        "    if isinstance(indices, torch.Tensor):\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "    collapsed = []\n",
        "    prev = None\n",
        "    for idx in indices:\n",
        "        if idx != prev:\n",
        "            collapsed.append(idx)\n",
        "        prev = idx\n",
        "\n",
        "    result = [i for i in collapsed if i != blank_index]\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "9fpw9CBqXNDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_cer = 0.0\n",
        "num_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets, input_lengths, target_lengths) in enumerate(val_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        # CTC Loss expects (T, N, C)\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        decoded_indices = torch.argmax(outputs, dim=2)  # (T, N)\n",
        "\n",
        "        T, N = decoded_indices.shape\n",
        "        for j in range(N):\n",
        "            pred_indices = decoded_indices[:, j].cpu().numpy()\n",
        "            pred_indices_decoded = ctc_decode(pred_indices, blank_index=0)\n",
        "            pred_text = int_sequence_to_text(pred_indices_decoded)\n",
        "\n",
        "            target_indices = targets[j].cpu().numpy()\n",
        "            target_length = target_lengths[j]\n",
        "            target_indices = target_indices[:target_length]\n",
        "            target_text = int_sequence_to_text([int(i) for i in target_indices])\n",
        "\n",
        "            current_cer = cer(pred_text, target_text)\n",
        "            total_cer += current_cer\n",
        "            num_samples += 1\n",
        "\n",
        "            if i == 0 and j < 3:\n",
        "                print(f\"Predicho: '{pred_text}'\")\n",
        "                print(f\"Real:     '{target_text}'\")\n",
        "                print(f\"CER: {current_cer:.4f}\")\n",
        "                print(\"---\")\n",
        "\n",
        "    print(f\"Average CER: {total_cer / num_samples:.4f}\")"
      ],
      "metadata": {
        "id": "CYZEYFgHTOWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencia"
      ],
      "metadata": {
        "id": "IZ6DMclzyxld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(audio_path, model, transform, device):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    max_val = torch.max(torch.abs(waveform))\n",
        "    if max_val > 0:\n",
        "      waveform = waveform/max_val\n",
        "\n",
        "    spectrogram = transform(waveform)\n",
        "    spectrogram = torch.log(spectrogram + 1e-6)\n",
        "    spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
        "    spectrogram = spectrogram.squeeze(0).transpose(0, 1)\n",
        "\n",
        "    #spectrogram = spectrogram.to(device)\n",
        "    spectrogram = spectrogram.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(spectrogram)\n",
        "        outputs = outputs.permute(1, 0, 2)  # (T, N, C)\n",
        "\n",
        "        decoded_indices = torch.argmax(outputs, dim=2)  # (T, N)\n",
        "        T = outputs.shape[0]\n",
        "        pred_indices = decoded_indices[:spectrogram.shape[1], 0].cpu().numpy()  # (T,)\n",
        "\n",
        "        pred_indices_decoded = ctc_decode(pred_indices, blank_index=0)\n",
        "        pred_text = int_sequence_to_text(pred_indices_decoded)\n",
        "        return pred_text"
      ],
      "metadata": {
        "id": "oVQMO8fYSN7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_audio_path = \"/content/test.flac\"  # Provide path to a .flac audio file\n",
        "test_audio_path = \"/content/CapturarImagen.wav\"\n",
        "predicted_text = predict(test_audio_path, model, transform, device)\n",
        "print(f\"Predicted Transcript: {predicted_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko1_YuI0zb4T",
        "outputId": "af88cc2c-7cc1-4098-dd5f-e323cf3d30a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Transcript: catura rigaf.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Kg09Dwz43Oxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportar el modelo"
      ],
      "metadata": {
        "id": "xbXMKX1Xb0uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos"
      ],
      "metadata": {
        "id": "0cDd6P4Tb5Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"speech_model_weights.pth\")\n",
        "\n",
        "# Cargar pesos\n",
        "#model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)\n",
        "#model.load_state_dict(torch.load(\"speech_model_weights.pth\", map_location=device))\n",
        "#model.eval()  # poner en modo evaluación"
      ],
      "metadata": {
        "id": "gkDH2_hkbyp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos + arquitectura"
      ],
      "metadata": {
        "id": "gLO2nqeccFFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar modelo completo\n",
        "torch.save(model, \"speech_model_full.pth\")\n",
        "\n",
        "# Cargar modelo completo\n",
        "#model = torch.load(\"speech_model_full.pth\", map_location=device)\n",
        "#model.eval()"
      ],
      "metadata": {
        "id": "wTkwaw3BcIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos + optimizador\n",
        "####(Para continuar el entrenamiento)"
      ],
      "metadata": {
        "id": "ma9h7MglcMqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar checkpoint\n",
        "torch.save({\n",
        "    'epoch': epoch,                     # opcional, para saber en qué época estás\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': running_loss,               # opcional\n",
        "}, \"checkpoint.pth\")\n",
        "\n",
        "# Reconstruir modelo y optimizador\n",
        "#model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#\n",
        "# Cargar checkpoint\n",
        "#checkpoint = torch.load(\"checkpoint.pth\", map_location=device)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#start_epoch = checkpoint['epoch'] + 1\n",
        "#running_loss = checkpoint['loss']\n",
        "#model.train()"
      ],
      "metadata": {
        "id": "3THRLduDcqw0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}