# -*- coding: utf-8 -*-
"""speech-to-text-transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fYGt1phCraIcCOFCSkAHVd8Qqee8witp
"""


from tokenizers import Tokenizer, models, pre_tokenizers, decoders
import re
import os
import torch
import torchaudio
import torchcodec
from torch.utils.data import Dataset, DataLoader
from torchaudio.transforms import MelSpectrogram
import torch.nn.functional as F
from datasets import load_dataset, concatenate_datasets

def get_tokenizer(save_path="tokenizer.json"):
  tokenizer = Tokenizer(models.BPE())
  tokenizer.add_special_tokens(["□"])
  tokenizer.add_tokens(list("ABCDEFGHIJKLMNÑOPQRSTUVWXYZÁÉÍÓÚÜ "))

  tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
  tokenizer.decoder = decoders.ByteLevel()
  tokenizer.blank_token = tokenizer.token_to_id("□")
  tokenizer.save(save_path)
  return tokenizer

def collate_fn(batch):
  max_audio_len = max(item["audio"].shape[0] for item in batch)

  max_ids_len = 0
  has_input_ids = "speaker_id" in batch[0]
  if has_input_ids:
    max_ids_len = max([len(item["speaker_id"]) for item in batch])

  audio_tensor = torch.stack([F.pad(item["audio"], (0, max_audio_len - item["audio"].shape[0])) for item in batch])

  output_dict = {
      "audio": audio_tensor,
      "text": [item["text"] for item in batch]
  }

  if has_input_ids:
    input_ids = torch.stack(
        [
            F.pad(
                torch.tensor(item["speaker_id"]),
                (0, max_ids_len - len(item["speaker_id"])),
                value=0
            ) for item in batch
        ]
    )
    output_dict["speaker_id"] = input_ids

  return output_dict

class CommonVoiceDataset(Dataset):
  def __init__(
      self,
      common_voice_dataset,
      num_examples=None,
      tokenizer=None,
  ):
    self.dataset = common_voice_dataset
    self.num_examples = (
        min(num_examples, len(common_voice_dataset))
        if num_examples is not None
        else len(common_voice_dataset)
    )
    self.tokenizer = tokenizer

  def __len__(self):
    return self.num_examples

  def __getitem__(self, idx):
    item = self.dataset[idx]
    waveform = torch.from_numpy(item["audio"]["array"]).float()
    text = item[
        "text"
    ].upper()
    if self.tokenizer:
      encoded = self.tokenizer.encode(text)
      return {"audio": waveform, "text": text, "speaker_id": encoded.ids}

    return {"audio": waveform, "text": text}

def get_dataset(batch_size=32, num_examples=None, num_workers=4):
  dataset_f = load_dataset("ylacombe/google-argentinian-spanish", "female")
  dataset_m = load_dataset("ylacombe/google-argentinian-spanish", "male")
  dataset = concatenate_datasets([dataset_f["train"], dataset_m["train"]])

  tokenizer = get_tokenizer()

  def clean_text(batch):
      text = batch["text"]

      text = re.sub(r"[¿?¡!]", "", text)

      text = re.sub(r"\s+", " ", text).strip()

      batch["text"] = text
      return batch

  dataset = dataset.map(clean_text)
  dataset = CommonVoiceDataset(dataset, tokenizer=tokenizer, num_examples=num_examples)

  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)
  return dataloader

if __name__ == "__main__":
    dataloader = get_dataset(batch_size=32)
    for batch in dataloader:
        audio = batch["audio"]
        input_ids = batch["speaker_id"]
        print(audio.shape)
        print(input_ids.shape)
        break

