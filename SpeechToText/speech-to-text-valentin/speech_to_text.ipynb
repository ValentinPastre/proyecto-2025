{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df1152e8201541958a740acb29a74021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78d62920558045c8ac780bb7a8285754",
              "IPY_MODEL_7bae53814f3f4de3bc3f7cd54ad216bb",
              "IPY_MODEL_c93fba15f9ee4b6cbc3451f5ef37ae9e"
            ],
            "layout": "IPY_MODEL_fed414ced97e4351a0ea3ffd52752b97"
          }
        },
        "78d62920558045c8ac780bb7a8285754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_595fd8f8af5b424284b58bab9125b230",
            "placeholder": "​",
            "style": "IPY_MODEL_72b5bdfb52c5445994a565323ae2a285",
            "value": "Map: 100%"
          }
        },
        "7bae53814f3f4de3bc3f7cd54ad216bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33854ca57bb5417f93ed51dc18103001",
            "max": 11569,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e5cf2ffbee84e4dbf4623ec8ae123ab",
            "value": 11569
          }
        },
        "c93fba15f9ee4b6cbc3451f5ef37ae9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebc4730dad774505b1220f0ff820f6d9",
            "placeholder": "​",
            "style": "IPY_MODEL_84522b0efcef4439bbfca8c3dc253803",
            "value": " 11569/11569 [00:14&lt;00:00, 1477.64 examples/s]"
          }
        },
        "fed414ced97e4351a0ea3ffd52752b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "595fd8f8af5b424284b58bab9125b230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b5bdfb52c5445994a565323ae2a285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33854ca57bb5417f93ed51dc18103001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e5cf2ffbee84e4dbf4623ec8ae123ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebc4730dad774505b1220f0ff820f6d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84522b0efcef4439bbfca8c3dc253803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib librosa torch torchaudio torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxVs_RjJFARL",
        "outputId": "49446c13-3986-4e5e-81e4-f12b0b64a423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n",
            "Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "cgXC07VweM9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "from sklearn.model_selection import train_test_split\n",
        "import soundfile as sf\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "uHXp6HI7FFJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "ds_f = load_dataset(\"ylacombe/google-argentinian-spanish\", \"female\")\n",
        "ds_m = load_dataset(\"ylacombe/google-argentinian-spanish\", \"male\")\n",
        "ds_wiki = load_dataset(\"ciempiess/wikipedia_spanish\")\n",
        "\n",
        "#ds = concatenate_datasets([ds_f[\"train\"], ds_m[\"train\"], ds_wiki[\"train\"]])\n",
        "ds = ds_wiki[\"train\"]\n",
        "\n",
        "print(ds.column_names)\n",
        "print(ds_f.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLTe8S7zFf_w",
        "outputId": "bd242c50-4501-471f-f070-1de35c8503c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['audio_id', 'audio', 'speaker_id', 'gender', 'duration', 'normalized_text']\n",
            "{'train': ['audio', 'text', 'speaker_id']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "ALLOWED_CHARS = \"abcdefghijklmnñopqrstuvwxyzáéíóúü \"\n",
        "\n",
        "def clean_text(batch):\n",
        "    #text = batch[\"text\"].lower()\n",
        "    text = batch[\"normalized_text\"].lower()\n",
        "\n",
        "    text = re.sub(r\"[¿?¡!.,:;\\-_/()\\[\\]\\\"']\", \" \", text)\n",
        "\n",
        "    text = re.sub(r\"[0-9]\", \"\", text)\n",
        "\n",
        "    text = \"\".join(ch for ch in text if ch in ALLOWED_CHARS)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    batch[\"text\"] = text\n",
        "    return batch\n",
        "\n",
        "ds = ds.map(clean_text)"
      ],
      "metadata": {
        "id": "C-fqppjCZayf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "df1152e8201541958a740acb29a74021",
            "78d62920558045c8ac780bb7a8285754",
            "7bae53814f3f4de3bc3f7cd54ad216bb",
            "c93fba15f9ee4b6cbc3451f5ef37ae9e",
            "fed414ced97e4351a0ea3ffd52752b97",
            "595fd8f8af5b424284b58bab9125b230",
            "72b5bdfb52c5445994a565323ae2a285",
            "33854ca57bb5417f93ed51dc18103001",
            "3e5cf2ffbee84e4dbf4623ec8ae123ab",
            "ebc4730dad774505b1220f0ff820f6d9",
            "84522b0efcef4439bbfca8c3dc253803"
          ]
        },
        "outputId": "c3ff53ea-10c4-4dcd-ad69-ad20035cebbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11569 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df1152e8201541958a740acb29a74021"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds)\n",
        "\n",
        "splits = ds.train_test_split(test_size=0.1, seed=55)\n",
        "train_df = splits[\"train\"]\n",
        "val_df = splits[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSnvzlh7IakP",
        "outputId": "7b0ec930-587b-47f8-ccdd-0a114a102df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['audio_id', 'audio', 'speaker_id', 'gender', 'duration', 'normalized_text', 'text'],\n",
            "    num_rows: 11569\n",
            "})\n",
            "Training samples: 10412, Validation samples: 1157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds[0][\"text\"])\n",
        "print(ds[7][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nul6F3dijQU",
        "outputId": "7b345739-2b59-4a34-b7b3-7c8068afdbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "donde revelaba de sus placas de vidrio al colodión controversias y equivocaciones\n",
            "que era ayer lo suyo en mil ochocientos ochenta y tres figuraba copropietaria de su archivo su hijastra catalina melina dosh rosbuak\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_chars = list(\"áéíóúüñ\")\n",
        "\n",
        "all_text = \" \".join(train_df['text']) + \" \" + \" \".join(val_df['text'])\n",
        "all_text = all_text.lower()\n",
        "\n",
        "dataset_chars = set(all_text)  # todos los que realmente aparecen\n",
        "allowed_chars = sorted(dataset_chars.union(extra_chars))\n",
        "\n",
        "char_map = {\"<BLANK>\": 0}\n",
        "for i, ch in enumerate(allowed_chars, start=1):\n",
        "    char_map[ch] = i\n",
        "\n",
        "index_map = {v: k for k, v in char_map.items()}"
      ],
      "metadata": {
        "id": "VwrdzF5LAeyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_int_sequence(text):\n",
        "    text = text.lower()\n",
        "    return [char_map.get(c, char_map[' ']) for c in text]\n",
        "\n",
        "def int_sequence_to_text(seq):\n",
        "    #return ''.join([index_map[i] for i in seq])\n",
        "    return ''.join([index_map.get(i, '') for i in seq])"
      ],
      "metadata": {
        "id": "n4onsy3xQJEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Custom Dataset"
      ],
      "metadata": {
        "id": "z06tsjT9eKCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augment = nn.Sequential(\n",
        "    T.FrequencyMasking(freq_mask_param=15),\n",
        "    T.TimeMasking(time_mask_param=35)\n",
        ")"
      ],
      "metadata": {
        "id": "DAksyphz4_Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, df, char_map, transform=None, augment=None, mean=None, std=None):\n",
        "        self.df = df\n",
        "        self.char_map = char_map\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.df[idx]\n",
        "        audio = item[\"audio\"][\"array\"]\n",
        "        sample_rate = item[\"audio\"][\"sampling_rate\"]\n",
        "        transcript = item[\"text\"]\n",
        "\n",
        "        waveform = torch.tensor(audio)\n",
        "        if waveform.dim() == 1:\n",
        "          waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Resample to 16kHz\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "            sample_rate = 16000\n",
        "\n",
        "        max_val = torch.max(torch.abs(waveform))\n",
        "        if max_val > 0:\n",
        "          waveform = waveform/max_val\n",
        "\n",
        "        spectrogram = self.transform(waveform)\n",
        "        spectrogram = torch.log(spectrogram + 1e-6)\n",
        "        if self.mean is not None and self.std is not None:\n",
        "          spectrogram = (spectrogram - self.mean) / (self.std + 1e-6)\n",
        "        #else:\n",
        "        #  spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
        "\n",
        "        if self.augment is not None:\n",
        "            spectrogram = self.augment(spectrogram)\n",
        "\n",
        "        spectrogram = spectrogram.squeeze(0).transpose(0, 1)\n",
        "\n",
        "        # Convert transcript to int sequence\n",
        "        transcript_seq = torch.tensor(text_to_int_sequence(transcript), dtype=torch.long)\n",
        "\n",
        "        return spectrogram, transcript_seq"
      ],
      "metadata": {
        "id": "XNVkLuLl7ubk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    spectrograms = []\n",
        "    transcript_seqs = []\n",
        "    input_lengths = []\n",
        "    target_lengths = []\n",
        "\n",
        "    for (spectrogram, transcript_seq) in batch:\n",
        "        if spectrogram.dim() != 2:\n",
        "            raise ValueError(f\"Expected spectrogram with 2 dims (T, n_mels), got {spectrogram.shape}\")\n",
        "\n",
        "        spectrograms.append(spectrogram)\n",
        "        transcript_seqs.append(transcript_seq)\n",
        "        input_lengths.append(spectrogram.shape[0])\n",
        "        target_lengths.append(len(transcript_seq))\n",
        "\n",
        "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
        "    transcript_seqs = torch.nn.utils.rnn.pad_sequence(transcript_seqs, batch_first=True)\n",
        "\n",
        "    return spectrograms, transcript_seqs, input_lengths, target_lengths"
      ],
      "metadata": {
        "id": "M2YDUcinag5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dataset_norm(loader):\n",
        "    total_sum = 0.0\n",
        "    total_sq_sum = 0.0\n",
        "    total_count = 0\n",
        "\n",
        "    for spectrograms, _, _, _ in loader:\n",
        "        # spectrograms shape: (B, T, M)\n",
        "        B, T_spectrograms_shape, M = spectrograms.shape\n",
        "        count = B * T_spectrograms_shape * M\n",
        "\n",
        "        total_sum += spectrograms.sum()\n",
        "        total_sq_sum += (spectrograms ** 2).sum()\n",
        "        total_count += count\n",
        "\n",
        "    mean = total_sum / total_count\n",
        "    var = (total_sq_sum / total_count) - mean**2\n",
        "    std = torch.sqrt(var)\n",
        "\n",
        "    return mean.item(), std.item()"
      ],
      "metadata": {
        "id": "x7DUoGrMIbjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "\n",
        "#estos train_tmp son una chanchada pero los hago para calcular la media y el desvio de todo el dataset\n",
        "train_dataset_tmp = SpeechDataset(train_df, char_map, transform=transform, augment=augment)\n",
        "train_loader_tmp = DataLoader(train_dataset_tmp, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "dataset_mean, dataset_std = compute_dataset_norm(train_loader_tmp)\n",
        "\n",
        "#train_dataset = SpeechDataset(train_df.reset_index(drop=True), char_map, transform=transform)\n",
        "#val_dataset = SpeechDataset(val_df.reset_index(drop=True), char_map, transform=transform)\n",
        "train_dataset = SpeechDataset(train_df, char_map, transform=transform, augment=augment, mean=dataset_mean, std=dataset_std)\n",
        "val_dataset = SpeechDataset(val_df, char_map, transform=transform, augment=None, mean=dataset_mean, std=dataset_std)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "FYmjG-ZTbc_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76222e7c-c0b2-4fb9-cfff-b4919713ad8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/functional/functional.py:582: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\"mean\": dataset_mean, \"std\": dataset_std}, \"dataset_norm.pth\")"
      ],
      "metadata": {
        "id": "XywaeSzcLalS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset mean: {dataset_mean}\")\n",
        "print(f\"Dataset std: {dataset_std}\")"
      ],
      "metadata": {
        "id": "zs_AciGxNt7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "8RDAtzMwd6-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        #self.conv = nn.Sequential(\n",
        "        #    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "        #    nn.BatchNorm2d(16),\n",
        "        #    nn.ReLU(),\n",
        "        #    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "        #    nn.BatchNorm2d(32),\n",
        "        #    nn.ReLU()\n",
        "        #)\n",
        "        #\n",
        "        #self.post_cnn_size = 32*input_size\n",
        "\n",
        "        #lstm original\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=3, bidirectional=True, batch_first=True)\n",
        "        #self.lstm = nn.LSTM(self.post_cnn_size, hidden_size, num_layers=3, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #codigo nuevo\n",
        "        #B, T, F_dim = x.shape\n",
        "        #x = x.permute(0, 2, 1).unsqueeze(1)\n",
        "        #x = self.conv(x)\n",
        "        #x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        #x = x.view(B, T, -1)\n",
        "        #fin codigo nuevo\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        x = F.log_softmax(x, dim=2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Byz2F9XVd-0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"   En Colab: Runtime , Change runtime type , T4 GPU\")\n",
        "\n",
        "input_size = 128  # Number of Mel features\n",
        "hidden_size = 512 #representa el estado interno, info que la red \"recuerda\" o \"aprende\"\n",
        "output_size = len(char_map)  # Number of characters\n",
        "\n",
        "model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)"
      ],
      "metadata": {
        "id": "WBbz6xVSebBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036a99ac-6ee0-4599-8723-664d22008a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "Memoria disponible: 14.74 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "_VdOtG94ekGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "j4o0z5nuegXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OJITO CON ESTO limpia la ram de la gpu\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1OpHyUyTtgQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 13  # Increase this number for better performance\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, targets, input_lengths, target_lengths) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        # CTC Loss expects (T, N, C)\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        input_lengths_tensor = torch.IntTensor(input_lengths)    # shape (N,)\n",
        "        target_lengths_tensor = torch.IntTensor(target_lengths)  # shape (N,)\n",
        "\n",
        "        targets_list = []\n",
        "        for t, l in zip(targets, target_lengths):\n",
        "            targets_list.append(t[:l])\n",
        "        targets_concat = torch.cat(targets_list).to(dtype=torch.long)  # 1D\n",
        "\n",
        "        outputs = outputs.to(device)               # (T, N, C)\n",
        "        targets_concat = targets_concat.to(device)\n",
        "        input_lengths_tensor = input_lengths_tensor.to(device)\n",
        "        target_lengths_tensor = target_lengths_tensor.to(device)\n",
        "\n",
        "        loss = criterion(outputs, targets_concat, input_lengths_tensor, target_lengths_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed with average loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "slt-T5otepfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d3e03f-924e-49f8-f186-aae22dd36cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Step 1/323, Loss: 0.4966\n",
            "Epoch 1/8, Step 11/323, Loss: 0.5037\n",
            "Epoch 1/8, Step 21/323, Loss: 0.4924\n",
            "Epoch 1/8, Step 31/323, Loss: 0.5853\n",
            "Epoch 1/8, Step 41/323, Loss: 0.5708\n",
            "Epoch 1/8, Step 51/323, Loss: 0.6218\n",
            "Epoch 1/8, Step 61/323, Loss: 0.5418\n",
            "Epoch 1/8, Step 71/323, Loss: 0.6364\n",
            "Epoch 1/8, Step 81/323, Loss: 0.5011\n",
            "Epoch 1/8, Step 91/323, Loss: 0.5240\n",
            "Epoch 1/8, Step 101/323, Loss: 0.4604\n",
            "Epoch 1/8, Step 111/323, Loss: 0.4205\n",
            "Epoch 1/8, Step 121/323, Loss: 0.5343\n",
            "Epoch 1/8, Step 131/323, Loss: 0.5241\n",
            "Epoch 1/8, Step 141/323, Loss: 0.5270\n",
            "Epoch 1/8, Step 151/323, Loss: 0.3961\n",
            "Epoch 1/8, Step 161/323, Loss: 0.6590\n",
            "Epoch 1/8, Step 171/323, Loss: 0.5021\n",
            "Epoch 1/8, Step 181/323, Loss: 0.4938\n",
            "Epoch 1/8, Step 191/323, Loss: 0.5208\n",
            "Epoch 1/8, Step 201/323, Loss: 0.3487\n",
            "Epoch 1/8, Step 211/323, Loss: 0.3850\n",
            "Epoch 1/8, Step 221/323, Loss: 0.4488\n",
            "Epoch 1/8, Step 231/323, Loss: 0.5597\n",
            "Epoch 1/8, Step 241/323, Loss: 0.4885\n",
            "Epoch 1/8, Step 251/323, Loss: 0.6384\n",
            "Epoch 1/8, Step 261/323, Loss: 0.5797\n",
            "Epoch 1/8, Step 271/323, Loss: 0.4910\n",
            "Epoch 1/8, Step 281/323, Loss: 0.4556\n",
            "Epoch 1/8, Step 291/323, Loss: 0.3931\n",
            "Epoch 1/8, Step 301/323, Loss: 0.4134\n",
            "Epoch 1/8, Step 311/323, Loss: 0.6005\n",
            "Epoch 1/8, Step 321/323, Loss: 0.5868\n",
            "Epoch 1 completed with average loss: 0.5019\n",
            "Epoch 2/8, Step 1/323, Loss: 0.5510\n",
            "Epoch 2/8, Step 11/323, Loss: 0.2637\n",
            "Epoch 2/8, Step 21/323, Loss: 0.3669\n",
            "Epoch 2/8, Step 31/323, Loss: 0.5136\n",
            "Epoch 2/8, Step 41/323, Loss: 0.5748\n",
            "Epoch 2/8, Step 51/323, Loss: 0.4724\n",
            "Epoch 2/8, Step 61/323, Loss: 0.4367\n",
            "Epoch 2/8, Step 71/323, Loss: 0.5790\n",
            "Epoch 2/8, Step 81/323, Loss: 0.4231\n",
            "Epoch 2/8, Step 91/323, Loss: 0.5705\n",
            "Epoch 2/8, Step 101/323, Loss: 0.4536\n",
            "Epoch 2/8, Step 111/323, Loss: 0.4622\n",
            "Epoch 2/8, Step 121/323, Loss: 0.5669\n",
            "Epoch 2/8, Step 131/323, Loss: 0.4984\n",
            "Epoch 2/8, Step 141/323, Loss: 0.4767\n",
            "Epoch 2/8, Step 151/323, Loss: 0.6379\n",
            "Epoch 2/8, Step 161/323, Loss: 0.4397\n",
            "Epoch 2/8, Step 171/323, Loss: 0.5139\n",
            "Epoch 2/8, Step 181/323, Loss: 0.5667\n",
            "Epoch 2/8, Step 191/323, Loss: 0.3615\n",
            "Epoch 2/8, Step 201/323, Loss: 0.3786\n",
            "Epoch 2/8, Step 211/323, Loss: 0.4594\n",
            "Epoch 2/8, Step 221/323, Loss: 0.4600\n",
            "Epoch 2/8, Step 231/323, Loss: 0.5461\n",
            "Epoch 2/8, Step 241/323, Loss: 0.4708\n",
            "Epoch 2/8, Step 251/323, Loss: 0.3932\n",
            "Epoch 2/8, Step 261/323, Loss: 0.3842\n",
            "Epoch 2/8, Step 271/323, Loss: 0.4170\n",
            "Epoch 2/8, Step 281/323, Loss: 0.3958\n",
            "Epoch 2/8, Step 291/323, Loss: 0.5063\n",
            "Epoch 2/8, Step 301/323, Loss: 0.4961\n",
            "Epoch 2/8, Step 311/323, Loss: 0.5372\n",
            "Epoch 2/8, Step 321/323, Loss: 0.5033\n",
            "Epoch 2 completed with average loss: 0.4683\n",
            "Epoch 3/8, Step 1/323, Loss: 0.5127\n",
            "Epoch 3/8, Step 11/323, Loss: 0.4299\n",
            "Epoch 3/8, Step 21/323, Loss: 0.7004\n",
            "Epoch 3/8, Step 31/323, Loss: 0.5796\n",
            "Epoch 3/8, Step 41/323, Loss: 0.4205\n",
            "Epoch 3/8, Step 51/323, Loss: 0.3396\n",
            "Epoch 3/8, Step 61/323, Loss: 0.4287\n",
            "Epoch 3/8, Step 71/323, Loss: 0.4643\n",
            "Epoch 3/8, Step 81/323, Loss: 0.4372\n",
            "Epoch 3/8, Step 91/323, Loss: 0.2774\n",
            "Epoch 3/8, Step 101/323, Loss: 0.3862\n",
            "Epoch 3/8, Step 111/323, Loss: 0.3733\n",
            "Epoch 3/8, Step 121/323, Loss: 0.3747\n",
            "Epoch 3/8, Step 131/323, Loss: 0.4368\n",
            "Epoch 3/8, Step 141/323, Loss: 0.4307\n",
            "Epoch 3/8, Step 151/323, Loss: 0.4639\n",
            "Epoch 3/8, Step 161/323, Loss: 0.5172\n",
            "Epoch 3/8, Step 171/323, Loss: 0.4829\n",
            "Epoch 3/8, Step 181/323, Loss: 0.3329\n",
            "Epoch 3/8, Step 191/323, Loss: 0.4016\n",
            "Epoch 3/8, Step 201/323, Loss: 0.4053\n",
            "Epoch 3/8, Step 211/323, Loss: 0.4336\n",
            "Epoch 3/8, Step 221/323, Loss: 0.4615\n",
            "Epoch 3/8, Step 231/323, Loss: 0.3994\n",
            "Epoch 3/8, Step 241/323, Loss: 0.3957\n",
            "Epoch 3/8, Step 251/323, Loss: 0.4487\n",
            "Epoch 3/8, Step 261/323, Loss: 0.3128\n",
            "Epoch 3/8, Step 271/323, Loss: 0.4835\n",
            "Epoch 3/8, Step 281/323, Loss: 0.4857\n",
            "Epoch 3/8, Step 291/323, Loss: 0.3379\n",
            "Epoch 3/8, Step 301/323, Loss: 0.3487\n",
            "Epoch 3/8, Step 311/323, Loss: 0.3553\n",
            "Epoch 3/8, Step 321/323, Loss: 0.4229\n",
            "Epoch 3 completed with average loss: 0.4330\n",
            "Epoch 4/8, Step 1/323, Loss: 0.3338\n",
            "Epoch 4/8, Step 11/323, Loss: 0.3067\n",
            "Epoch 4/8, Step 21/323, Loss: 0.3881\n",
            "Epoch 4/8, Step 31/323, Loss: 0.3393\n",
            "Epoch 4/8, Step 41/323, Loss: 0.3986\n",
            "Epoch 4/8, Step 51/323, Loss: 0.3666\n",
            "Epoch 4/8, Step 61/323, Loss: 0.3811\n",
            "Epoch 4/8, Step 71/323, Loss: 0.4571\n",
            "Epoch 4/8, Step 81/323, Loss: 0.3891\n",
            "Epoch 4/8, Step 91/323, Loss: 0.3543\n",
            "Epoch 4/8, Step 101/323, Loss: 0.3859\n",
            "Epoch 4/8, Step 111/323, Loss: 0.4877\n",
            "Epoch 4/8, Step 121/323, Loss: 0.2963\n",
            "Epoch 4/8, Step 131/323, Loss: 0.5159\n",
            "Epoch 4/8, Step 141/323, Loss: 0.5258\n",
            "Epoch 4/8, Step 151/323, Loss: 0.5928\n",
            "Epoch 4/8, Step 161/323, Loss: 0.3730\n",
            "Epoch 4/8, Step 171/323, Loss: 0.3319\n",
            "Epoch 4/8, Step 181/323, Loss: 0.4216\n",
            "Epoch 4/8, Step 191/323, Loss: 0.3386\n",
            "Epoch 4/8, Step 201/323, Loss: 0.3521\n",
            "Epoch 4/8, Step 211/323, Loss: 0.3595\n",
            "Epoch 4/8, Step 221/323, Loss: 0.3993\n",
            "Epoch 4/8, Step 231/323, Loss: 0.4682\n",
            "Epoch 4/8, Step 241/323, Loss: 0.2786\n",
            "Epoch 4/8, Step 251/323, Loss: 0.3234\n",
            "Epoch 4/8, Step 261/323, Loss: 0.3067\n",
            "Epoch 4/8, Step 271/323, Loss: 0.7620\n",
            "Epoch 4/8, Step 281/323, Loss: 0.3998\n",
            "Epoch 4/8, Step 291/323, Loss: 0.4079\n",
            "Epoch 4/8, Step 301/323, Loss: 0.4377\n",
            "Epoch 4/8, Step 311/323, Loss: 0.2813\n",
            "Epoch 4/8, Step 321/323, Loss: 0.5015\n",
            "Epoch 4 completed with average loss: 0.4087\n",
            "Epoch 5/8, Step 1/323, Loss: 0.4365\n",
            "Epoch 5/8, Step 11/323, Loss: 0.4439\n",
            "Epoch 5/8, Step 21/323, Loss: 0.3054\n",
            "Epoch 5/8, Step 31/323, Loss: 0.3324\n",
            "Epoch 5/8, Step 41/323, Loss: 0.4177\n",
            "Epoch 5/8, Step 51/323, Loss: 0.3722\n",
            "Epoch 5/8, Step 61/323, Loss: 0.3726\n",
            "Epoch 5/8, Step 71/323, Loss: 0.3326\n",
            "Epoch 5/8, Step 81/323, Loss: 0.2928\n",
            "Epoch 5/8, Step 91/323, Loss: 0.5053\n",
            "Epoch 5/8, Step 101/323, Loss: 0.4838\n",
            "Epoch 5/8, Step 111/323, Loss: 0.4006\n",
            "Epoch 5/8, Step 121/323, Loss: 0.2911\n",
            "Epoch 5/8, Step 131/323, Loss: 0.3571\n",
            "Epoch 5/8, Step 141/323, Loss: 0.2564\n",
            "Epoch 5/8, Step 151/323, Loss: 0.3029\n",
            "Epoch 5/8, Step 161/323, Loss: 0.4390\n",
            "Epoch 5/8, Step 171/323, Loss: 0.4812\n",
            "Epoch 5/8, Step 181/323, Loss: 0.3798\n",
            "Epoch 5/8, Step 191/323, Loss: 0.4284\n",
            "Epoch 5/8, Step 201/323, Loss: 0.4162\n",
            "Epoch 5/8, Step 211/323, Loss: 0.3153\n",
            "Epoch 5/8, Step 221/323, Loss: 0.3454\n",
            "Epoch 5/8, Step 231/323, Loss: 0.4369\n",
            "Epoch 5/8, Step 241/323, Loss: 0.3251\n",
            "Epoch 5/8, Step 251/323, Loss: 0.3640\n",
            "Epoch 5/8, Step 261/323, Loss: 0.3139\n",
            "Epoch 5/8, Step 271/323, Loss: 0.3894\n",
            "Epoch 5/8, Step 281/323, Loss: 0.4381\n",
            "Epoch 5/8, Step 291/323, Loss: 0.4188\n",
            "Epoch 5/8, Step 301/323, Loss: 0.4938\n",
            "Epoch 5/8, Step 311/323, Loss: 0.4125\n",
            "Epoch 5/8, Step 321/323, Loss: 0.3605\n",
            "Epoch 5 completed with average loss: 0.3868\n",
            "Epoch 6/8, Step 1/323, Loss: 0.2994\n",
            "Epoch 6/8, Step 11/323, Loss: 0.3809\n",
            "Epoch 6/8, Step 21/323, Loss: 0.4124\n",
            "Epoch 6/8, Step 31/323, Loss: 0.3235\n",
            "Epoch 6/8, Step 41/323, Loss: 0.3277\n",
            "Epoch 6/8, Step 51/323, Loss: 0.2909\n",
            "Epoch 6/8, Step 61/323, Loss: 0.3389\n",
            "Epoch 6/8, Step 71/323, Loss: 0.3345\n",
            "Epoch 6/8, Step 81/323, Loss: 0.3131\n",
            "Epoch 6/8, Step 91/323, Loss: 0.3816\n",
            "Epoch 6/8, Step 101/323, Loss: 0.3251\n",
            "Epoch 6/8, Step 111/323, Loss: 0.3921\n",
            "Epoch 6/8, Step 121/323, Loss: 0.2455\n",
            "Epoch 6/8, Step 131/323, Loss: 0.3026\n",
            "Epoch 6/8, Step 141/323, Loss: 0.2857\n",
            "Epoch 6/8, Step 151/323, Loss: 0.4017\n",
            "Epoch 6/8, Step 161/323, Loss: 0.4068\n",
            "Epoch 6/8, Step 171/323, Loss: 0.3556\n",
            "Epoch 6/8, Step 181/323, Loss: 0.3266\n",
            "Epoch 6/8, Step 191/323, Loss: 0.3075\n",
            "Epoch 6/8, Step 201/323, Loss: 0.4469\n",
            "Epoch 6/8, Step 211/323, Loss: 0.3307\n",
            "Epoch 6/8, Step 221/323, Loss: 0.2750\n",
            "Epoch 6/8, Step 231/323, Loss: 0.4452\n",
            "Epoch 6/8, Step 241/323, Loss: 0.3910\n",
            "Epoch 6/8, Step 251/323, Loss: 0.3253\n",
            "Epoch 6/8, Step 261/323, Loss: 0.3386\n",
            "Epoch 6/8, Step 271/323, Loss: 0.3779\n",
            "Epoch 6/8, Step 281/323, Loss: 0.4059\n",
            "Epoch 6/8, Step 291/323, Loss: 0.2685\n",
            "Epoch 6/8, Step 301/323, Loss: 0.2661\n",
            "Epoch 6/8, Step 311/323, Loss: 0.3062\n",
            "Epoch 6/8, Step 321/323, Loss: 0.3932\n",
            "Epoch 6 completed with average loss: 0.3552\n",
            "Epoch 7/8, Step 1/323, Loss: 0.4071\n",
            "Epoch 7/8, Step 11/323, Loss: 0.2103\n",
            "Epoch 7/8, Step 21/323, Loss: 0.3067\n",
            "Epoch 7/8, Step 31/323, Loss: 0.3355\n",
            "Epoch 7/8, Step 41/323, Loss: 0.4184\n",
            "Epoch 7/8, Step 51/323, Loss: 0.3628\n",
            "Epoch 7/8, Step 61/323, Loss: 0.3290\n",
            "Epoch 7/8, Step 71/323, Loss: 0.5006\n",
            "Epoch 7/8, Step 81/323, Loss: 0.3695\n",
            "Epoch 7/8, Step 91/323, Loss: 0.2329\n",
            "Epoch 7/8, Step 101/323, Loss: 0.2336\n",
            "Epoch 7/8, Step 111/323, Loss: 0.2831\n",
            "Epoch 7/8, Step 121/323, Loss: 0.3101\n",
            "Epoch 7/8, Step 131/323, Loss: 0.3786\n",
            "Epoch 7/8, Step 141/323, Loss: 0.4099\n",
            "Epoch 7/8, Step 151/323, Loss: 0.2613\n",
            "Epoch 7/8, Step 161/323, Loss: 0.3162\n",
            "Epoch 7/8, Step 171/323, Loss: 0.2754\n",
            "Epoch 7/8, Step 181/323, Loss: 0.4148\n",
            "Epoch 7/8, Step 191/323, Loss: 0.2562\n",
            "Epoch 7/8, Step 201/323, Loss: 0.5889\n",
            "Epoch 7/8, Step 211/323, Loss: 0.3751\n",
            "Epoch 7/8, Step 221/323, Loss: 0.3073\n",
            "Epoch 7/8, Step 231/323, Loss: 0.2849\n",
            "Epoch 7/8, Step 241/323, Loss: 0.3701\n",
            "Epoch 7/8, Step 251/323, Loss: 0.3532\n",
            "Epoch 7/8, Step 261/323, Loss: 0.2936\n",
            "Epoch 7/8, Step 271/323, Loss: 0.3436\n",
            "Epoch 7/8, Step 281/323, Loss: 0.3456\n",
            "Epoch 7/8, Step 291/323, Loss: 0.3519\n",
            "Epoch 7/8, Step 301/323, Loss: 0.3206\n",
            "Epoch 7/8, Step 311/323, Loss: 0.4351\n",
            "Epoch 7/8, Step 321/323, Loss: 0.3473\n",
            "Epoch 7 completed with average loss: 0.3401\n",
            "Epoch 8/8, Step 1/323, Loss: 0.2140\n",
            "Epoch 8/8, Step 11/323, Loss: 0.2981\n",
            "Epoch 8/8, Step 21/323, Loss: 0.2099\n",
            "Epoch 8/8, Step 31/323, Loss: 0.3581\n",
            "Epoch 8/8, Step 41/323, Loss: 0.2138\n",
            "Epoch 8/8, Step 51/323, Loss: 0.2217\n",
            "Epoch 8/8, Step 61/323, Loss: 0.2435\n",
            "Epoch 8/8, Step 71/323, Loss: 0.2763\n",
            "Epoch 8/8, Step 81/323, Loss: 0.3359\n",
            "Epoch 8/8, Step 91/323, Loss: 0.2781\n",
            "Epoch 8/8, Step 101/323, Loss: 0.3342\n",
            "Epoch 8/8, Step 111/323, Loss: 0.2944\n",
            "Epoch 8/8, Step 121/323, Loss: 0.3580\n",
            "Epoch 8/8, Step 131/323, Loss: 0.3798\n",
            "Epoch 8/8, Step 141/323, Loss: 0.2412\n",
            "Epoch 8/8, Step 151/323, Loss: 0.2460\n",
            "Epoch 8/8, Step 161/323, Loss: 0.3112\n",
            "Epoch 8/8, Step 171/323, Loss: 0.3853\n",
            "Epoch 8/8, Step 181/323, Loss: 0.2429\n",
            "Epoch 8/8, Step 191/323, Loss: 0.2643\n",
            "Epoch 8/8, Step 201/323, Loss: 0.2118\n",
            "Epoch 8/8, Step 211/323, Loss: 0.2899\n",
            "Epoch 8/8, Step 221/323, Loss: 0.2889\n",
            "Epoch 8/8, Step 231/323, Loss: 0.2984\n",
            "Epoch 8/8, Step 241/323, Loss: 0.3012\n",
            "Epoch 8/8, Step 251/323, Loss: 0.3678\n",
            "Epoch 8/8, Step 261/323, Loss: 0.3177\n",
            "Epoch 8/8, Step 271/323, Loss: 0.2260\n",
            "Epoch 8/8, Step 281/323, Loss: 0.3152\n",
            "Epoch 8/8, Step 291/323, Loss: 0.3269\n",
            "Epoch 8/8, Step 301/323, Loss: 0.3799\n",
            "Epoch 8/8, Step 311/323, Loss: 0.3473\n",
            "Epoch 8/8, Step 321/323, Loss: 0.2601\n",
            "Epoch 8 completed with average loss: 0.3118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate the model"
      ],
      "metadata": {
        "id": "J54lqfL9oDEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import editdistance\n",
        "\n",
        "def cer(pred, ref):\n",
        "    return editdistance.eval(pred, ref) / len(ref)"
      ],
      "metadata": {
        "id": "13sC7loSFJ75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_decode(indices, blank_index=0):\n",
        "    \"\"\"\n",
        "    indices: tensor shape (T,) con los índices ya argmaxeados.\n",
        "    \"\"\"\n",
        "    if isinstance(indices, torch.Tensor):\n",
        "        indices = indices.cpu().numpy()\n",
        "\n",
        "    collapsed = []\n",
        "    prev = None\n",
        "    for idx in indices:\n",
        "        if idx != prev:\n",
        "            collapsed.append(idx)\n",
        "        prev = idx\n",
        "\n",
        "    result = [i for i in collapsed if i != blank_index]\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "2M4bCeFn0XdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_cer = 0.0\n",
        "num_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets, input_lengths, target_lengths) in enumerate(val_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        # CTC Loss expects (T, N, C)\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        decoded_indices = torch.argmax(outputs, dim=2)  # (T, N)\n",
        "\n",
        "        T, N = decoded_indices.shape\n",
        "        for j in range(N):\n",
        "            pred_indices = decoded_indices[:, j].cpu().numpy()\n",
        "            pred_indices_decoded = ctc_decode(pred_indices, blank_index=0)\n",
        "            pred_text = int_sequence_to_text(pred_indices_decoded)\n",
        "\n",
        "            target_indices = targets[j].cpu().numpy()\n",
        "            target_length = target_lengths[j]\n",
        "            target_indices = target_indices[:target_length]\n",
        "            target_text = int_sequence_to_text([int(i) for i in target_indices])\n",
        "\n",
        "            current_cer = cer(pred_text, target_text)\n",
        "            total_cer += current_cer\n",
        "            num_samples += 1\n",
        "\n",
        "            if i == 0 and j < 7:\n",
        "                print(f\"Predicho: '{pred_text}'\")\n",
        "                print(f\"Real:     '{target_text}'\")\n",
        "                print(f\"CER: {current_cer:.4f}\")\n",
        "                print(\"---\")\n",
        "\n",
        "    print(f\"Average CER: {total_cer / num_samples:.4f}\")"
      ],
      "metadata": {
        "id": "CYZEYFgHTOWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56dfbf60-0dac-4cc2-e161-22549f33b3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicho: 'tengo muchos discos de vinilo'\n",
            "Real:     'tengo muchos discos de vinilo'\n",
            "CER: 0.0000\n",
            "---\n",
            "Predicho: 'está muy cansado'\n",
            "Real:     'está muy cansado'\n",
            "CER: 0.0000\n",
            "---\n",
            "Predicho: 'la ciudad de a capuco estás s sores de colectivo'\n",
            "Real:     'la ciudad de acapulco está a seis horas de colectivo'\n",
            "CER: 0.1731\n",
            "---\n",
            "Predicho: 'me gustaría escuchar les mejor escanciones de roc desde los sesenta hastá ros noventa'\n",
            "Real:     'me gustaría escuchar las mejores canciones de rock desde los sesenta a los noventa'\n",
            "CER: 0.1098\n",
            "---\n",
            "Predicho: 'monte video estábicadaen el rígo due la plata'\n",
            "Real:     'montevideo está ubicada en el río de la plata'\n",
            "CER: 0.1333\n",
            "---\n",
            "Predicho: 'me pudés decir que máquina describir de bo comprar'\n",
            "Real:     'me puedes decir que máquina de escribir debo comprar'\n",
            "CER: 0.0962\n",
            "---\n",
            "Predicho: 'necesito un martillo y varios clavos para coldar bin el cuedro'\n",
            "Real:     'necesito un martillo y varios clavos para colgar bien el cuadro'\n",
            "CER: 0.0476\n",
            "---\n",
            "Average CER: 0.0993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencia"
      ],
      "metadata": {
        "id": "IZ6DMclzyxld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(audio_path, model, transform, device):\n",
        "    stats = torch.load(\"dataset_norm.pth\", map_location=device)\n",
        "    dataset_mean = stats[\"mean\"]\n",
        "    dataset_std = stats[\"std\"]\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    max_val = torch.max(torch.abs(waveform))\n",
        "    if max_val > 0:\n",
        "      waveform = waveform/max_val\n",
        "\n",
        "    spectrogram = transform(waveform)\n",
        "    spectrogram = torch.log(spectrogram + 1e-6)\n",
        "    #spectrogram = (spectrogram - spectrogram.mean()) / (spectrogram.std() + 1e-6)\n",
        "    spectrogram = (spectrogram - dataset_mean) / (dataset_std + 1e-6)\n",
        "    spectrogram = spectrogram.squeeze(0).transpose(0, 1)\n",
        "\n",
        "    #spectrogram = spectrogram.to(device)\n",
        "    spectrogram = spectrogram.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(spectrogram)\n",
        "        outputs = outputs.permute(1, 0, 2)  # (T, N, C)\n",
        "\n",
        "        decoded_indices = torch.argmax(outputs, dim=2)  # (T, N)\n",
        "        time = outputs.shape[0] #time es T\n",
        "        #pred_indices = decoded_indices[:spectrogram.shape[1], 0].cpu().numpy()  # (T,)\n",
        "        pred_indices = decoded_indices[:  , 0].cpu().numpy()\n",
        "\n",
        "        pred_indices_decoded = ctc_decode(pred_indices, blank_index=0)\n",
        "        pred_text = int_sequence_to_text(pred_indices_decoded)\n",
        "        return pred_text"
      ],
      "metadata": {
        "id": "oVQMO8fYSN7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_dbfs(waveform):\n",
        "    if isinstance(waveform, np.ndarray):\n",
        "        waveform = torch.tensor(waveform, dtype=torch.float32)\n",
        "    else:\n",
        "        waveform = waveform.float()\n",
        "\n",
        "    if waveform.dim() > 1:\n",
        "        waveform = waveform.mean(dim=0)\n",
        "\n",
        "    rms = torch.sqrt(torch.mean(waveform ** 2))\n",
        "\n",
        "    dbfs = 20 * torch.log10(rms + 1e-12)\n",
        "\n",
        "    return dbfs.item()\n"
      ],
      "metadata": {
        "id": "Vx5zKMolLBJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_audio_path = \"/content/test.flac\"  # Provide path to a .flac audio file\n",
        "test_audio_path = \"/content/RepetirPassword.wav\"\n",
        "\n",
        "audio_dataset = train_df[0][\"audio\"][\"array\"]\n",
        "waveform, sample_rate = torchaudio.load(test_audio_path)\n",
        "\n",
        "print(f\"Decibeles del dataset: {calculate_dbfs(audio_dataset)}\")\n",
        "print(f\"Decibeles del audio a predecir: {calculate_dbfs(waveform)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF2y4ehPLMBa",
        "outputId": "d13e760d-7c78-4bef-f656-48765b52af4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decibeles del dataset: -28.09765625\n",
            "Decibeles del audio a predecir: -25.864051818847656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_audio_path = \"/content/test.flac\"  # Provide path to a .flac audio file\n",
        "predicted_text = predict(test_audio_path, model, transform, device)\n",
        "print()\n",
        "print(f\"Predicted Transcript: {predicted_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko1_YuI0zb4T",
        "outputId": "da29b201-538a-48a9-8708-800ff258c46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Transcript: r es petie pontra leño\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportar el modelo"
      ],
      "metadata": {
        "id": "xbXMKX1Xb0uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos"
      ],
      "metadata": {
        "id": "0cDd6P4Tb5Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"speech_model_weights.pth\")\n",
        "\n",
        "# Cargar pesos\n",
        "#model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)\n",
        "#model.load_state_dict(torch.load(\"speech_model_weights.pth\", map_location=device))\n",
        "#model.eval()  # poner en modo evaluación"
      ],
      "metadata": {
        "id": "gkDH2_hkbyp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos + arquitectura"
      ],
      "metadata": {
        "id": "gLO2nqeccFFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar modelo completo\n",
        "torch.save(model, \"speech_model_full.pth\")\n",
        "\n",
        "# Cargar modelo completo\n",
        "#model = torch.load(\"speech_model_full.pth\", map_location=device)\n",
        "#model.eval()"
      ],
      "metadata": {
        "id": "wTkwaw3BcIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesos + optimizador\n",
        "####(Para continuar el entrenamiento)"
      ],
      "metadata": {
        "id": "ma9h7MglcMqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar checkpoint\n",
        "torch.save({\n",
        "    'epoch': epoch,                     # opcional, para saber en qué época estás\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': running_loss,               # opcional\n",
        "}, \"checkpoint.pth\")\n",
        "\n",
        "# Reconstruir modelo y optimizador\n",
        "#model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Cargar checkpoint\n",
        "#checkpoint = torch.load(\"checkpoint.pth\", map_location=device)\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#start_epoch = checkpoint['epoch'] + 1\n",
        "#running_loss = checkpoint['loss']\n",
        "#model.train()"
      ],
      "metadata": {
        "id": "3THRLduDcqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruir modelo y optimizador\n",
        "model = SpeechRecognitionModel(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Cargar checkpoint\n",
        "checkpoint = torch.load(\"checkpoint.pth\", map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "running_loss = checkpoint['loss']\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmOYzl0qBtPs",
        "outputId": "05784124-e506-473c-b9b3-a37faa01cd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpeechRecognitionModel(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (lstm): LSTM(4096, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=1024, out_features=35, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}